{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA learns logical Operator AND\n",
    "\n",
    "This is an example of an Artificial Intelligence learning the AND operator.\n",
    "We are using a multiperceptron.\n",
    "\n",
    "- A perceptron is a NN (Neural Network) were you have the entries and one layer which is the output one.\n",
    "- On a multiperceptron, we have hidden layers appart from the output ine and the entries.\n",
    "- Also, is a supervised model where we are going to give the model what output should it take, and applaying backpropagation, calculating it's gradient decent and updating the hyperparameters.\n",
    "\n",
    "    NN with hidden layer   |           Neuron         \n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/NN_hidden.png)  | ![](images/neuron.png)\n",
    "\n",
    "\n",
    "#### 1ยบ Data set\n",
    "| e1| e2| s |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 1 | 1 |\n",
    "\n",
    "#### 2ยบ Activation function\n",
    "\n",
    "Sigmoidal in each neuron, we could try some others, like linear, ReLU (Rectified Linear Unit) or tanh.\n",
    "\n",
    "#### 3ยบ Model\n",
    "\n",
    "This model is going to have just one hidden layer, output layer and 2 entries (e1, e2).\n",
    "Is a supervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "# constants\n",
    "LEARNING_RATE = 1.5\n",
    "N_LAYERS = 2 # 1 hidden layer and 1 output layer, the entry one does not count as a layer\n",
    "N_NEURONS_HIDDEN_LAYER = 3\n",
    "N_NEURONS_ENTRIES = 2\n",
    "N_NEURONS_OUTPUT = 1\n",
    "\n",
    "# data sets\n",
    "\n",
    "##### train case #####\n",
    "\n",
    "# train_set_X -> 2 inputs and 4 train examples\n",
    "train_set_X = np.array([\n",
    "    # np.reshape([0, 0], (1, 2)),\n",
    "    np.reshape([0, 1], (1, 2)),\n",
    "    np.reshape([1, 0], (1, 2)),\n",
    "    np.reshape([1, 1], (1, 2)),\n",
    "])\n",
    "print(train_set_X[0].shape)\n",
    "\n",
    "train_set_Y = np.reshape([\n",
    "    # 0,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "], (3, 1))\n",
    "\n",
    "##### test case #####\n",
    "\n",
    "test_set_X = np.array([\n",
    "    np.reshape([0, 0], (1, 2)),\n",
    "    np.reshape([1, 0], (1, 2)),\n",
    "    np.reshape([1, 1], (1, 2)),\n",
    "])\n",
    "\n",
    "test_set_Y = np.reshape([\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "], (1, 3))\n",
    "\n",
    "##### hyperparameters (W, b) #####\n",
    "# bias from entries is 0\n",
    "# b0 = 0\n",
    "\n",
    "# first layer weights\n",
    "# [first entry, second entry]\n",
    "w1 = np.array([\n",
    "    [np.random.uniform(0, 0.5), np.random.uniform(0, 0.5)], # first neuron from hidden layer\n",
    "    [np.random.uniform(0.3, 0.6), np.random.uniform(0.3, 0.6)], # second neuron from hidden layer\n",
    "    [np.random.uniform(-1, -0.5), np.random.uniform(-1, -0.5)], # third neuron from hidden layer\n",
    "])\n",
    "\n",
    "b1 = np.reshape([\n",
    "    np.random.uniform(-0.2, 0), # np.random.uniform(0, 1),\n",
    "    np.random.uniform(-1, -0.5),\n",
    "    np.random.uniform(0, 1),\n",
    "], (1, 3))\n",
    "\n",
    "print(b1)\n",
    "\n",
    "w2 = np.reshape([\n",
    "    np.random.uniform(-1, -0.5), # np.random.uniform(0, 1), # first neuron from hidden layer\n",
    "    np.random.uniform(-0.5, 0), # second neuron from hidden layer\n",
    "    np.random.uniform(0.5, 1), # third neuron from hidden layer\n",
    "], (1, 3))\n",
    "\n",
    "b2 = np.reshape(np.random.uniform(0, 0.5), (1, 1)) # np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function and its derivative\n",
    "\n",
    "- Sigmoid: $f(x) = \\frac{1}{1+\\mathrm{e}^{-x}}$\n",
    "- Sigmoid Derivate: $f'(x) = f(x)*(1-f(x))$\n",
    "\n",
    "The derivate is use to calculate how much does our model need to \"rectificate\", later you will see implement a function $\\delta$ that implements it.\n",
    "\n",
    "<img\n",
    "     src=\"https://cdn-images-1.medium.com/max/1600/0*rBCtCuf6-L5kYHGE.png\"\n",
    "     style=\"width:350px;height:150px;\"\n",
    "/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "Here we implement the forward propagation. It is the way to calculate the output, so in this step we have to calculate the activation function. (in this case)\n",
    "1. Calculate the linear function with W (Weight), b (bias) and e (entries) in the first layer (which is the hidden layer)\n",
    "2. Apply activation function (sigmoid) with the last output\n",
    "3. Calculate the linear function of the last layer (output layer) as in the first step, but with a little difference, here we need to apply W (from the last layer) instead of e (entries)\n",
    "4. Apply activation function\n",
    "\n",
    "<img src=\"images/NN_forward_prop.png\" style=\"width:500px;height:300px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, w, b):\n",
    "    # weigth from bias is 1\n",
    "    # calculate f(x) and a(x), we can do this by:\n",
    "    # try to uncomment this to see shapes print(X.shape, w.T.shape, b.shape)\n",
    "    return sigmoid(np.matmul(X, w.T)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aclaration\n",
    "\n",
    "When we have layers, we want to get the size of the **Neurons** you have in the actual layer with the **Neurons** from the next layer. Example of the NN in the img.\n",
    "\n",
    "<img src=\"images/forward_prop_steps.png\" style=\"width:550px;height:350px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example for the forwarding for 2 layers.\n",
    "\n",
    "# We get an array with dimensions (1, 3)\n",
    "activation_func1 = forward_propagation(train_set_X[0], w1, b1)\n",
    "print(activation_func1.shape, activation_func1.squeeze())\n",
    "# We get an array with dimensions (1, 1)\n",
    "activation_func2 = forward_propagation(activation_func1, w2, b2)\n",
    "print(activation_func2.shape, activation_func2.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "In this step we have the output **s** and the desire output **d**, to calculate how much the hyperparameters (w, b) has to change to get the output we want, we do the next step, called _Backward propagation_, here do the next:\n",
    "\n",
    "1. Calculate $\\delta^{(k)} = (d - s^{(k)}) * s^{(k)} * (1-s^{(k)})$ from last layer. Here we calculate the error.\n",
    "2. Calculate $\\Delta w$ for the last layer ($\\Delta w = \\alpha * s^{(k-1).T} * \\delta^{(k)}$) it gives us how we need to change the weights to be more accurate for the desire output. (last layer)\n",
    "3. Calculate $\\Delta b$ for the last layer ($\\Delta b = \\alpha * \\delta^{(k)}$)\n",
    "4. Update weights $w^{(k)}(t+1) = w^{(k)}(t) + \\delta w^{(k)}(t+1)$\n",
    "5. Update bias $b^{(k)}(t+1) = b^{(k)}(t) + \\delta b^{(k)}(t+1)$\n",
    "Now the same with the hidden layer, but in a different way, because we don't have a desire output\n",
    "6. Calculate $V^{(k)} = \\delta^{(k+1)} * w^{(k+1)}$ the error in the last layer.\n",
    "7. Calculate $\\delta^{(k)} = V^{(k)} * s^{k} * (1-s^{(k)})$ the error each neuron of current layer.\n",
    "8. Calculate $\\Delta w$ for the last layer ($\\Delta w = \\alpha * s^{(k-1).T} * \\delta^{(k)}$) how much we change the neurons weights on the hidden layers.\n",
    "9. Calculate $\\Delta b$ for the last layer ($\\Delta b = \\alpha * \\delta^{(k)}$)\n",
    "10. Update weights $w^{(k)}(t+1) = w^{(k)}(t) + \\delta w^{(k)}(t+1)$\n",
    "11. Update bias $b^{(k)}(t+1) = b^{(k)}(t) + \\delta b^{(k)}(t+1)$\n",
    "\n",
    "from 6 to 7 has to be repeted for each hidden layer.\n",
    "\n",
    "- k -> layer\n",
    "- s -> activation function output\n",
    "- d -> desire output\n",
    "- t -> number of times the model has learned\n",
    "- $\\alpha$ -> learning_rate\n",
    "- $\\delta$ -> lowercase delta\n",
    "- $\\Delta$ -> uppercase delta\n",
    "\n",
    "remmember $s^0$ are the entries.\n",
    "\n",
    "<img src=\"images/NN_backward_prop.png\" style=\"width:550px;height:450px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I know sounds like a lot, but wish with the img you can see it better\n",
    "# now what we need to do is the function back_prop, here we generalize all from above\n",
    "def back_prop(s, w=None, Y=None, last_err=None, learning_rate=LEARNING_RATE):\n",
    "    # Y -- desire output, in hidden layers, we pass the activation function from the last layer\n",
    "    # s -- s[0] <- current activation function\n",
    "    #      s[1] <- last activation function (last because is the layer which is back to the current one)\n",
    "    \n",
    "    # remember, last activation function, since there is only one neuron, we have no matrix\n",
    "    # other wise we need to do a matrix multiplication.\n",
    "    err = 0\n",
    "    # this means we are on the last layer\n",
    "    if Y != None:\n",
    "        # lower delta\n",
    "        err = (Y-s[0]) * s[0] * (1-s[0])\n",
    "    else:\n",
    "        v = np.dot(last_err, w)\n",
    "        err = np.dot(np.dot(v, s[0].T), (1-s[0]))\n",
    "\n",
    "    del_w = np.dot(learning_rate * s[1].T, err).T\n",
    "    del_b = learning_rate * err\n",
    "    \n",
    "    hyperparams = {\n",
    "        'err': err,\n",
    "        'del_w': del_w,\n",
    "        'del_b': del_b,\n",
    "    }\n",
    "    \n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with the forward prop in the last cells\n",
    "# we had e1=e2=0, and the deire output is 0\n",
    "\n",
    "print('---------------- output layer ----------------\\n')\n",
    "\n",
    "# last layer\n",
    "s = [activation_func2, activation_func1]\n",
    "err2 = back_prop(s, Y=0)\n",
    "print(f\"error: {err2['err'].shape} \\n\", err2['err'])\n",
    "print(f\"\\ndel_w: {err2['del_w'].shape} \\n\", err2['del_w'])\n",
    "print(f\"\\ndel_b: {err2['del_b'].shape} \\n\", err2['del_b'])\n",
    "\n",
    "print('---------------- hidden layer ----------------\\n')\n",
    "\n",
    "# hidden layer\n",
    "# this part should be a loop so that we cover every hidden layer with back prop, but since it is an example and we \n",
    "# just have one layer, we are going to omit it.\n",
    "s = [activation_func1, train_set_X[0]]\n",
    "err1 = back_prop(s, w=w2, last_err=err2['err'])\n",
    "print(f\"error: {err1['err'].shape} \\n\", err1['err'])\n",
    "print(f\"\\ndel_w: {err1['del_w'].shape} \\n\", err1['del_w'])\n",
    "print(f\"\\ndel_b: {err1['del_b'].shape} \\n\", err1['del_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update hyperparameters\n",
    "def update(w, del_w, b, del_b):\n",
    "    return  w + del_w, b + del_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_hyper = [[{\"w2\": w2, \"b2\": b2}],[{\"w1\": w1, \"b1\": b1}]]\n",
    "\n",
    "print(f'Old hyperparams\\n- w2: {w2}\\n- b2: {b2}\\n- w1: {w1}\\n- b1: {b1}')\n",
    "\n",
    "# update second layer\n",
    "w2_ex, b2_ex = update(err2['del_w'], w2, err2['del_b'], b2)\n",
    "history_hyper[0].append({\"w2\": w2_ex, \"b2\": b2_ex})\n",
    "w1_ex, b1_ex = update(err1['del_w'], w1, err1['del_b'], b1)\n",
    "history_hyper[1].append({\"w1\": w1_ex, \"b1\": b1_ex})\n",
    "\n",
    "print(f'\\nNew hyperparams\\n- w2: {w2_ex}\\n- b2: {b2_ex}\\n- w1: {w1_ex}\\n- b1: {b1_ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this is done just for this model, but it can be generalize\n",
    "def print_l(history_hyper):\n",
    "    # ---------------------------------- #\n",
    "    output_layer = history_hyper[0]\n",
    "    # 3 connections with the output layer.\n",
    "    w2 = [[output_layer[0][\"w2\"].squeeze()[0]], [output_layer[0][\"w2\"].squeeze()[1]], [output_layer[0][\"w2\"].squeeze()[2]]]\n",
    "    b2 = [output_layer[0][\"b2\"].squeeze().tolist()]\n",
    "\n",
    "    for i in range(1, len(output_layer)):\n",
    "        for j in range(len(output_layer[i][\"w2\"].squeeze())):\n",
    "            w2[j].append(output_layer[i][\"w2\"].squeeze()[j])\n",
    "\n",
    "        b2.append(output_layer[i][\"b2\"].squeeze().tolist())\n",
    "\n",
    "    # ---------------------------------- #\n",
    "    hidden_layer = history_hyper[1]\n",
    "    w1 = [[] for _ in range(N_NEURONS_HIDDEN_LAYER*N_NEURONS_ENTRIES)]\n",
    "    b1 = [[] for _ in range(N_NEURONS_HIDDEN_LAYER)]\n",
    "    # 6 weights (6 connections from entries to neurons, so we have 2 entries for each neuron [3 neurons] => 6 connections)\n",
    "    # 3 bias from each neuron of the hidden layer\n",
    "    n_weights = 0\n",
    "    n_bias = 0\n",
    "\n",
    "    for i in range(len(hidden_layer)):\n",
    "        for j in hidden_layer[i][\"w1\"]:\n",
    "            for elem in j:\n",
    "                if n_weights >= 6:\n",
    "                    n_weights = 0\n",
    "                w1[n_weights].append(elem)\n",
    "                n_weights += 1\n",
    "        for j in hidden_layer[i][\"b1\"]:\n",
    "            for elem in j:\n",
    "                if n_bias >= 3:\n",
    "                    n_bias = 0\n",
    "                b1[n_bias].append(elem)\n",
    "                n_bias += 1\n",
    "\n",
    "    # ---------------------------------- #\n",
    "    output_layer_fig, output_layer_ax = plt.subplots(1, len(w2), figsize=(15, len(w2)), sharey=False)\n",
    "    for i in range(len(w2)):  \n",
    "        output_layer_ax[i].plot(w2[i], label=f\"W neuron {i}\")\n",
    "        output_layer_ax[i].legend(loc=\"upper right\")\n",
    "    output_layer_fig.suptitle('Weights from hidden layer, from 0 to 2nd layer')\n",
    "\n",
    "    _, output_layer_ax_bias = plt.subplots(figsize=(4.3, 3), sharey=False)\n",
    "    output_layer_ax_bias.plot(b2, label=\"bias from output neuron\")\n",
    "    output_layer_ax_bias.legend()\n",
    "\n",
    "    hidden_layer_fig, hidden_layer_ax = plt.subplots(1, len(w1), figsize=(15*len(w1)/2, len(w1)), sharey=False)\n",
    "    for i in range(len(w1)):\n",
    "        hidden_layer_ax[i].plot(w1[i], label=f\"W neuron {i}\")\n",
    "        hidden_layer_ax[i].legend(loc=\"upper right\")\n",
    "    hidden_layer_fig.suptitle('Weights from entry layer to hidden layers neurons, from 0 to 2ns hidden layer neurons')\n",
    "\n",
    "    hidden_layer_fig_bias, hidden_layer_ax_bias = plt.subplots(1, 3, figsize=(15, 3), sharey=False)\n",
    "    for i in range(len(b1)):\n",
    "        hidden_layer_ax_bias[i].plot(b1[i], label=f\"bias hidden layer neuron {i}\")\n",
    "        hidden_layer_ax_bias[i].legend(loc=\"upper right\")\n",
    "    hidden_layer_fig_bias.suptitle('Bias from hidden layer neurons, from 0 to 2st')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Just for you to see how graphicaly the weights and bias has changed. [the example]\n",
    "print_l(history_hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO model\n",
    "all group\n",
    "learning X number of epochs and all that stuff\n",
    "\n",
    "## Grouping forward and backward making the model\n",
    "\n",
    "At this point we have implemented:\n",
    "- Activation function ($\\sigma$)\n",
    "- Forward propagation\n",
    "- Backward propagation\n",
    "\n",
    "What the model has to do:\n",
    "1. Train\n",
    "    1. **Forward propagation** with a data set\n",
    "    2. **Backward propagation**, learn from its result\n",
    "    3. **Update** hyperparameters\n",
    "2. Test\n",
    "    1. **Predict**, this means, given a situation, with what the NN has learned give its output.\n",
    "    2. **Accuracy**, some times we want to know how well our NN is doing, we do this by comparing the outputs with desired outputs, depending on how many outputs has correct, its accuracy is better or worst.\n",
    "\n",
    "Step one (Train) is done X time, those X times are the number, of epochs (when you train 1 time the NN with the whole data set) you train the whole data set.\n",
    "    \n",
    "_Note: A, B, C should be 1, 2, 3_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, w, b, Y, epochs=10, print_progress=False, plot_progress=False, plt_all_progress=True, learning_rate=LEARNING_RATE):\n",
    "    # X - training data set (entries)\n",
    "    # w - weights\n",
    "    #     + w[0] - first layer weights\n",
    "    #     + w[n] - next n layer weights\n",
    "    # b - bias from training data set\n",
    "    #     + b[0] - first layer bias\n",
    "    #     + b[n] - next n layer bias\n",
    "    # Y - desire output data set\n",
    "    # epochs - number of repetitions\n",
    "    # print_progress - print all the progress from each data set\n",
    "    # plot_progress - plot in a graph the progress\n",
    "\n",
    "    # Some history if you need it \n",
    "    # activation_func_history = [[] for _ in range(len(X))]\n",
    "    # err_history = [[] for _ in range(len(X))]\n",
    "    \n",
    "    history_hyper = [[{\"w\"+str(i): w[i-1], \"b\"+str(i): b[i-1]}] for i in range(len(w), 0, -1)]\n",
    "\n",
    "    # Training\n",
    "    for i in range(epochs):\n",
    "        if print_progress:\n",
    "            print(f\"----------------------- epoch {i+1} -----------------------\")\n",
    "        for j in range(len(X)): # j gives us the training set we are using\n",
    "\n",
    "            # ---------- Forward propagation with each layer ---------- #\n",
    "            s = []\n",
    "            # first from entry to first hidden layer\n",
    "            s.append(forward_propagation(X[j], w[0], b[0]))\n",
    "            # from each hidden layer that exists to the output layer\n",
    "            for layer in range(N_LAYERS-1):\n",
    "                s.append(forward_propagation(s[layer], w[layer+1], b[layer+1]))\n",
    "\n",
    "            #activation_func_history[j].append(s)\n",
    "            \n",
    "            # now we all outputs from each neuron -> s\n",
    "            #if print_progress:\n",
    "            #    print(s)\n",
    "            #    print(f\"------------- activation_func_history [{j}] -------------\")\n",
    "            #    print(activation_func_history[j])\n",
    "            #    print(\"----------------------------------------------------------\")\n",
    "\n",
    "            # ---------- Backward propagation from output to entries ---------- #\n",
    "            # first we need to calculate the error from the desire output\n",
    "            # remmember, here we have the output layer in the first position, not as in the forward prop (s)\n",
    "            err = []\n",
    "            err.append(back_prop([s[N_LAYERS-1], s[N_LAYERS-2]], Y=Y[j], learning_rate=learning_rate))\n",
    "            # next step is to calculate the error each neuron has done\n",
    "            for layer in range(N_LAYERS-2, 0, -1): # from N_LAYERS-2 to 0\n",
    "                # N_LAYERS-2 because we did the first step (-1) before, now the next one (-1)\n",
    "                err.append(back_prop([s[layer], s[layer+1]], w=w[layer+1], last_err=err[layer+1]['err'], learning_rate=learning_rate))\n",
    "\n",
    "            # last, and more important, calculate the error from weights of entry layer\n",
    "            err.append(back_prop([s[0], X[j]], w=w[1], last_err=err[0]['err']))\n",
    "            \n",
    "            #err_history[j].append(err)\n",
    "            \n",
    "            #if print_progress:\n",
    "            #    print(err)\n",
    "            #    print(f\"------------- activation_func_history [{j}] -------------\")\n",
    "            #    print(err_history[j])\n",
    "            #    print(\"----------------------------------------------------------\")\n",
    "                \n",
    "            # ---------- Update values ---------- #\n",
    "            # remmember, when we store the error of each layer, we store it from output layer to the first one\n",
    "            # and w, b is the other way\n",
    "            for i in range(N_LAYERS):\n",
    "                weight, bias = update(w[i], err[N_LAYERS-i-1]['del_w'], b[i], err[N_LAYERS-i-1]['del_b'])\n",
    "                w[i] = weight\n",
    "                b[i] = bias\n",
    "                history_hyper[N_LAYERS-i-1].append({f\"w{i+1}\": weight, f\"b{i+1}\": bias})\n",
    "                \n",
    "        if plot_progress:\n",
    "            print_l(history_hyper)\n",
    "    \n",
    "    if plt_all_progress:\n",
    "        print_l(history_hyper)\n",
    "    \n",
    "    w_b = []\n",
    "    #print(len(history_hyper[0]))\n",
    "    #print(history_hyper[0][len(history_hyper[0])-1])\n",
    "    for i in range(N_LAYERS):\n",
    "        # print(f\"w{N_LAYERS-i}\", history_hyper[i][len(history_hyper[i])-1][f\"w{N_LAYERS-i}\"])\n",
    "        w_b.append({\n",
    "            f\"w{N_LAYERS-i}\": history_hyper[i][len(history_hyper[i])-1][f\"w{N_LAYERS-i}\"],\n",
    "            f\"b{N_LAYERS-i}\": history_hyper[i][len(history_hyper[i])-1][f\"b{N_LAYERS-i}\"],\n",
    "        })\n",
    "\n",
    "    return w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w = [w1, w2]\n",
    "b = [b1, b2]\n",
    "print(w)\n",
    "print(b)\n",
    "hyper_params_after = model(train_set_X, w, b, train_set_Y, epochs=500, plt_all_progress=True, learning_rate=1.5)\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "for i in range(N_LAYERS):\n",
    "    w[N_LAYERS-i-1] = hyper_params_after[i][f\"w{N_LAYERS-i}\"]\n",
    "    b[N_LAYERS-i-1] = hyper_params_after[i][f\"b{N_LAYERS-i}\"]\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_X\n",
    "# test_set_Y\n",
    "def predict(X, w, b):\n",
    "    s = []\n",
    "    # first from entry to first hidden layer\n",
    "    s.append(forward_propagation(X, w[0], b[0]))\n",
    "    # from each hidden layer that exists to the output layer\n",
    "    for layer in range(N_LAYERS-1):\n",
    "        s.append(forward_propagation(s[layer], w[layer+1], b[layer+1]))\n",
    "\n",
    "    print(s[N_LAYERS-1])\n",
    "\n",
    "    return (1 if s[N_LAYERS-1] > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, desire_outputs):\n",
    "    correct = 0\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == desire_outputs.squeeze()[i]:\n",
    "            correct+=1\n",
    "\n",
    "    return (correct/len(outputs))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [predict(test_set_X[0], w, b), predict(test_set_X[1], w, b), predict(test_set_X[2], w, b)]\n",
    "\n",
    "print(f'{test_set_X[0].squeeze()[0]} * {test_set_X[0].squeeze()[1]} = {predictions[0]}') # 0\n",
    "print(f'{test_set_X[1].squeeze()[0]} * {test_set_X[1].squeeze()[1]} = {predictions[1]}') # 0\n",
    "print(f'{test_set_X[2].squeeze()[0]} * {test_set_X[2].squeeze()[1]} = {predictions[2]}') # 1\n",
    "\n",
    "acc = accuracy(predictions, test_set_Y)\n",
    "\n",
    "print(f\"Accuracy: {acc}% from {len(predictions)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is all!!!\n",
    "\n",
    "Hope you understand all.\n",
    "\n",
    "Also, you can use this model with the OR logic, and try with XOR!!!\n",
    "\n",
    "Don't forget to change values!! and play with it!!\n",
    "Try adding a tanh as activation function, or another one!\n",
    "\n",
    "Give me a Star , that helps me to continue doing these things.\n",
    "\n",
    "Have a good day, and see you!!\n",
    "\n",
    "Made by: **@blitty-codes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just one more thing, for you to notice how important the data set you are training with is the biggest thing. Try combinations like:\n",
    "\n",
    "\n",
    "------------------------------\n",
    "```\n",
    "train_set_X = np.array([\n",
    "    # np.reshape([0, 0], (1, 2)),\n",
    "    # np.reshape([0, 1], (1, 2)),\n",
    "    np.reshape([1, 0], (1, 2)),\n",
    "    np.reshape([1, 1], (1, 2)),\n",
    "])\n",
    "print(train_set_X[0].shape)\n",
    "\n",
    "train_set_Y = np.reshape([\n",
    "    # 0,\n",
    "    # 0,\n",
    "    0,\n",
    "    1,\n",
    "], (3, 1))\n",
    "\n",
    "------------------------------\n",
    "train_set_X = np.array([\n",
    "    # np.reshape([0, 0], (1, 2)),\n",
    "    np.reshape([0, 1], (1, 2)),\n",
    "    np.reshape([1, 0], (1, 2)),\n",
    "    # np.reshape([1, 1], (1, 2)),\n",
    "])\n",
    "print(train_set_X[0].shape)\n",
    "\n",
    "train_set_Y = np.reshape([\n",
    "    # 0,\n",
    "    0,\n",
    "    0,\n",
    "    # 1,\n",
    "], (3, 1))\n",
    "\n",
    "------------------------------\n",
    "train_set_X = np.array([\n",
    "    np.reshape([0, 0], (1, 2)),\n",
    "    # np.reshape([0, 1], (1, 2)),\n",
    "    np.reshape([1, 0], (1, 2)),\n",
    "    np.reshape([1, 1], (1, 2)),\n",
    "])\n",
    "print(train_set_X[0].shape)\n",
    "\n",
    "train_set_Y = np.reshape([\n",
    "    0,\n",
    "    # 0,\n",
    "    0,\n",
    "    1,\n",
    "], (3, 1))\n",
    "\n",
    "------------------------------\n",
    "train_set_X = np.array([\n",
    "    # np.reshape([0, 0], (1, 2)),\n",
    "    np.reshape([0, 1], (1, 2)),\n",
    "    np.reshape([1, 0], (1, 2)),\n",
    "    np.reshape([1, 1], (1, 2)),\n",
    "])\n",
    "print(train_set_X[0].shape)\n",
    "\n",
    "train_set_Y = np.reshape([\n",
    "    # 0,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "], (3, 1))\n",
    "```\n",
    "------------------------------\n",
    "and run all cells, you'll see how the cuallity of the data and the cuantity matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
